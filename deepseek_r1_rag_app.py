# -*- coding: utf-8 -*-
"""DeepSeek-r1--Rag--App

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LCwGep8iLkbafbfhwfdVJKoJvabnKvnB
"""

!pip install langgraph langchain pinecone langchain_community langchain_groq pypdf langchain_pinecone

#setting up the api keys
from google.colab import userdata
DeepSeekR1=userdata.get('groq_api_key')
PineCone=userdata.get('pine_cone')
google_api_key = userdata.get('GOOGLE_API_KEY')

import os
os.environ['GROQ_API_KEY']=DeepSeekR1
os.environ['PINECONE_API_KEY']=PineCone
os.environ["GOOGLE_API_KEY"] = google_api_key

"""**## #in my case i will use deepseekr1 and i will use the database pinecone** **to store the embeddings**"""

#import dependencies
import langchain
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import pinecone
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores.pinecone import Pinecone
from langchain.memory import ConversationBufferWindowMemory
import pinecone

#setup the llms
from langchain_groq import ChatGroq
Model = ChatGroq(
    model = "deepseek-r1-distill-llama-70b",
    temperature=0,
    api_key = DeepSeekR1
)

#setup the pinecone
from pinecone import Pinecone,ServerlessSpec
pc=Pinecone(api_key=PineCone)

index_name = "test-database0"

pc.create_index(
    name = index_name,
    dimension =  768,
    metric = "cosine",
    spec = ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)

index = pc.Index(index_name) #TO connect the database

#prepare to load the document
document = PyPDFLoader("/content/DeepSeek_R1.pdf")
data = document.load()

def TextSplitter(text):
  text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
    add_start_index = True,
  )
  splitter = text_splitter.split_documents(text)
  return splitter

text = TextSplitter(data)

text

#load embeddings
from langchain.embeddings import HuggingFaceBgeEmbeddings
embeddings = HuggingFaceBgeEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2",
)

#Now stroes the embeddings into pinecone
from langchain_pinecone import PineconeVectorStore
vector_from_documents = PineconeVectorStore.from_documents(
    documents=text,
    embedding=embeddings,
    index_name=index_name,
)

#test with query and only get top 2 ressults

retriver = vector_from_documents.as_retriever(search_kwargs={"k": 1})

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm=Model,
    chain_type="stuff",
    retriever=retriver,
    # return_source_documents=True,
)

query = "Benchmark of deepseek r1?"

qa_chain.invoke(query)

